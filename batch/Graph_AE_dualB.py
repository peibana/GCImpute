# åŒå‘batch


import logging
import os

logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)

import numpy as np
import torch
from torch import optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

import get_adj
import utils
import model

import loss






def Graph_AE_handler(X_dropout, args, param):

    logging.info('--------> Starting Graph plus AE (cells Ã— genes dual batching, sparse subgraph) ...')

    device = param['device']
    use_GAT = args.graph_AE_use_GAT
    learning_rate = args.graph_AE_learning_rate
    factor = args.graph_AE_factor
    total_epoch = args.graph_AE_epoch
    patience = args.graph_AE_patience

    dropout_gene = args.graph_Gene_dropout
    gat_multi_heads_gene = args.graph_Gene_gat_multi_heads
    multi_heads_gene = args.ae_multi_heads_gene
    hid_embed1_gene = args.graph_Gene_hid_embed1
    hid_embed2_gene = args.graph_Gene_hid_embed2

    dropout_cell = args.graph_Cell_dropout
    gat_multi_heads_cell = args.graph_Cell_gat_multi_heads
    multi_heads_cell = args.ae_multi_heads_cell
    hid_embed1_cell = args.graph_Cell_hid_embed1
    hid_embed2_cell = args.graph_Cell_hid_embed2

    # åŒå‘ batch å°ºå¯¸
    Bc = args.graph_AE_cell_batch_size      # e.g. 512/1024
    Bg = args.graph_AE_gene_batch_size      # e.g. 2048/4096
    logging.info(f"[Batch Config] Cell batch size (Bc): {Bc}, Gene batch size (Bg): {Bg}")

    use_checkpoint = args.use_checkpoint

    output_dir = os.path.join(args.output_dir, 'impute')
    os.makedirs(output_dir, exist_ok=True)
    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    gene_adj_file = os.path.join(output_dir, "adj_gene.pt")
    cell_adj_file = os.path.join(output_dir, "adj_cell.pt")

    # ===================== å…¨å±€é‚»æŽ¥ï¼ˆä¸€æ¬¡æ€§æž„å»º/è¯»å–ï¼‰ =====================
    if os.path.exists(gene_adj_file) and os.path.exists(cell_adj_file):
        logging.info("âœ… Found cached adjacency matrices, loading from file ...")
        adj_gene_csr = torch.load(gene_adj_file)
        adj_cell_csr = torch.load(cell_adj_file)
        logging.info(f"Loaded gene adjacency {type(adj_gene_csr)} | cell adjacency {type(adj_cell_csr)}")
    else:
        logging.info("âš¡ Cached adjacency not found, computing adjacency matrices ...")
        # gene å…¨å±€é‚»æŽ¥ï¼ˆscipy.csrï¼‰
        adj_gene_csr = get_adj.get_gene_adjacency_matrix(X_dropout)  # (GÃ—G) csr
        # cell å…¨å±€é‚»æŽ¥ï¼ˆscipy.csrï¼‰
        adj_cell_csr = get_adj.get_cell_adjacency_matrix(X_dropout)  # (NÃ—N) csr

        # ä¿å­˜ç¼“å­˜
        torch.save(adj_gene_csr, gene_adj_file)
        torch.save(adj_cell_csr, cell_adj_file)
        logging.info("âœ… Adjacency matrices computed and saved for future runs.")

    # æ•°æ®çŸ©é˜µï¼ˆnumpyï¼‰
    X_all = X_dropout.X
    N, G = X_all.shape

    graph_AE = model.Graph_AE(
        N, G,
        hid_embed1_gene, hid_embed2_gene, dropout_gene, gat_multi_heads_gene, multi_heads_gene,
        hid_embed1_cell, hid_embed2_cell, dropout_cell, gat_multi_heads_cell, multi_heads_cell, use_checkpoint
    ).to(device)

    optimizer = optim.SGD(graph_AE.parameters(), lr=learning_rate)

    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, verbose=True)

    # ç»“æžœå®¹å™¨
    X_recon_full = torch.zeros((N, G), dtype=torch.float32)
    PI_full      = torch.zeros((N, G), dtype=torch.float32)
    THETA_full   = torch.zeros((N, G), dtype=torch.float32)

    best_X_recon_full = torch.zeros((N, G), dtype=torch.float32)  # CPU
    best_PI_full = torch.zeros((N, G), dtype=torch.float32)  # CPU
    best_THETA_full = torch.zeros((N, G), dtype=torch.float32)  # CPU

    # æ‰“ä¹±ç´¢å¼•
    cell_indices = np.arange(N)
    gene_indices = np.arange(G)

    PI, X_recon = None, None

    best_epoch, min_loss = 1, float('inf')
    graph_AE_loss_list = []

    # ðŸ”¥ è®­ç»ƒ + æ˜¾å­˜ç›‘æŽ§
    epoch_peaks = []
    for epoch in range(total_epoch):
        if param['device'].type == "cuda":
            torch.cuda.reset_peak_memory_stats(param['device'])

        graph_AE.train()

        total_epoch_loss = 0.0

        # æ¯ä¸ª epoch æ‰“ä¹±
        if getattr(args, 'shuffle_each_epoch', True):
            np.random.shuffle(cell_indices)
            np.random.shuffle(gene_indices)

        # åŒé‡å¾ªçŽ¯ï¼šcells Ã— genes
        for c_start in range(0, N, Bc):
            c_end = min(c_start + Bc, N)
            idx_c = cell_indices[c_start:c_end]             # (Bc,)

            # é¢„è£å‰ª cell å­å›¾ï¼ˆç”¨äºŽæ¯ä¸ª gene å—å¤ç”¨ï¼Œé¿å…é‡å¤ csr åˆ‡ç‰‡ï¼‰
            cell_sub_csr = adj_cell_csr[idx_c][:, idx_c]    # csr (BcÃ—Bc)

            # é¢„æž„é€  cell å›¾ï¼ˆGCN/GATï¼‰
            if use_GAT:
                graph_cell_sub = utils.csr_sub_to_edge_index(cell_sub_csr, device)
            else:
                graph_cell_sub = utils.csr_sub_to_torch_sparse(cell_sub_csr, device)

            # cell å›¾æŸå¤±æ ‡ç­¾ï¼ˆdenseï¼‰
            adj_cell_label_sub = torch.from_numpy(cell_sub_csr.toarray()).float().to(device)
            pos_weight_cell_sub, norm_cell_sub = utils.subgraph_posweight_norm(cell_sub_csr, device)

            # å– cell å­å—çš„è¡¨è¾¾
            X_c = X_all[idx_c]   # (Bc Ã— G) numpy

            for g_start in range(0, G, Bg):
                g_end = min(g_start + Bg, G)
                idx_g = gene_indices[g_start:g_end]        # (Bg,)

                # ---------- å– (cell, gene) å­çŸ©é˜µ ----------
                # X_sub: (Bc Ã— Bg)
                X_sub_np = X_c[:, idx_g]
                X_sub = torch.from_numpy(X_sub_np).float().to(device)

                # gene å­å›¾ï¼ˆç”¨äºŽ GNN_gene ä¸Žå›¾æŸå¤±ï¼‰
                gene_sub_csr = adj_gene_csr[idx_g][:, idx_g]   # csr (BgÃ—Bg)
                if use_GAT:
                    graph_gene_sub = utils.csr_sub_to_edge_index(gene_sub_csr, device)
                else:
                    graph_gene_sub = utils.csr_sub_to_torch_sparse(gene_sub_csr, device)

                # gene å›¾æŸå¤±æ ‡ç­¾ï¼ˆdenseï¼‰
                adj_gene_label_sub = torch.from_numpy(gene_sub_csr.toarray()).float().to(device)
                pos_weight_gene_sub, norm_gene_sub = utils.subgraph_posweight_norm(gene_sub_csr, device)

                # ====== å•ä¸ªå­å—ä¼˜åŒ– ======
                optimizer.zero_grad(set_to_none=True)

                # å‰å‘
                adj_gene_recon, adj_gene_info, adj_cell_recon, adj_cell_info, X_recon, PI, THETA = graph_AE(
                    X_sub, graph_gene_sub, graph_cell_sub, idx_c=idx_c, idx_g=idx_g, use_GAT=use_GAT
                )

                # AE æŸå¤±ï¼ˆåªå¯¹å½“å‰å­å—ï¼‰
                lzinbloss = loss.LZINBLoss()
                ae_loss = lzinbloss(X_sub, X_recon, PI, THETA)

                # Graph æŸå¤±ï¼šå¯¹ gene å­å›¾ä¸Ž cell å­å›¾åˆ†åˆ«æ¯”å¯¹å…¶å­æ ‡ç­¾
                if use_GAT:
                    graph_loss = loss.loss_function_GAT(
                        preds=(adj_gene_recon, adj_cell_recon),
                        labels=(adj_gene_label_sub, adj_cell_label_sub)
                    )
                else:
                    graph_loss = loss.loss_function_GCN(
                        preds=(adj_gene_recon, adj_cell_recon),
                        labels=(adj_gene_label_sub, adj_cell_label_sub),
                        num_nodes_gene=graph_gene_sub.shape[0] if graph_gene_sub.is_sparse else adj_gene_label_sub.shape[0],
                        pos_weight_gene=pos_weight_gene_sub,
                        norm_gene=norm_gene_sub,
                        adj_gene_info1=adj_gene_info[0], adj_gene_info2=adj_gene_info[1],
                        num_nodes_cell=adj_cell_label_sub.shape[0],
                        pos_weight_cell=pos_weight_cell_sub,
                        norm_cell=norm_cell_sub,
                        adj_cell_info1=adj_cell_info[0], adj_cell_info2=adj_cell_info[1]
                    )

                batch_loss = ae_loss + graph_loss
                # ====== åå‘ä¼ æ’­ ======
                if torch.isnan(batch_loss):
                    logging.warning(
                        f"âš ï¸ NaN loss detected in epoch {epoch + 1}, batch (c:{c_start}-{c_end}, g:{g_start}-{g_end}), skipping.")
                    optimizer.zero_grad(set_to_none=True)
                    continue

                batch_loss.backward()
                utils.log_system_usage(device, note=f" Epoch {epoch + 1}, c:{c_start}-{c_end}, g:{g_start}-{g_end}")

                # âœ… æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(graph_AE.parameters(), max_norm=5.0)

                optimizer.step()
                total_epoch_loss += batch_loss.item()

                # ====== å°†å­å—å†™å›ž CPU å…¨çŸ©é˜µå¯¹åº”ä½ç½® ======
                # X_recon / PI: (Bc Ã— Bg)
                # ====== å°†å­å—å†™å›ž CPU å…¨çŸ©é˜µå¯¹åº”ä½ç½® ======
                with torch.no_grad():
                    X_recon_full[np.ix_(idx_c, idx_g)] = X_recon.detach().cpu()
                    PI_full[np.ix_(idx_c, idx_g)] = PI.detach().cpu()
                    THETA_full[np.ix_(idx_c, idx_g)] = THETA.detach().cpu()

                # æ˜¾å­˜æ¸…ç†ï¼ˆä¿å®ˆï¼‰
                del X_sub, X_recon, PI, THETA, \
                    gene_sub_csr, adj_gene_label_sub, \
                    graph_gene_sub
                torch.cuda.empty_cache()

            # é‡Šæ”¾ cell å—ç›¸å…³
            del X_c, cell_sub_csr, adj_cell_label_sub, graph_cell_sub
            torch.cuda.empty_cache()

        lr_scheduler.step(total_epoch_loss)

        logging.info(f"----------------> Epoch: {epoch + 1}/{total_epoch}, Current loss: {total_epoch_loss:.6f}")
        graph_AE_loss_list.append(total_epoch_loss)

        if total_epoch_loss < min_loss:
            min_loss = total_epoch_loss
            best_epoch = epoch + 1
            torch.save(graph_AE.state_dict(), os.path.join(output_dir, 'Graph_AE_best_model.pkl'))

            # ðŸ”¥ åŒæ—¶ä¿å­˜å½“ä¸‹æœ€ä¼˜çš„ç»“æžœ
            best_X_recon_full = X_recon_full.clone()
            best_PI_full = PI_full.clone()
            best_THETA_full = THETA_full.clone()

        if param['device'].type == "cuda":
            peak_epoch_mem = torch.cuda.max_memory_allocated(param['device']) / 1024 ** 2
            epoch_peaks.append(peak_epoch_mem)
            utils.log_gpu_memory(param['device'], note=f"Epoch {epoch + 1} peak")

        utils.log_system_usage(device, note=f" Epoch {epoch + 1} end")

    # =============== è®­ç»ƒå®Œæˆï¼šé€‰æ‹©æ€§æ’è¡¥å¹¶è¿”å›ž ===============
    param['epoch_peaks'] = epoch_peaks
    param['graph_AE_loss_list'] = graph_AE_loss_list

    print('Best epoch:', best_epoch)
    print('Min loss:', min_loss)

    # é€‰æ‹©æ€§æ’è¡¥
    X_cpu = torch.from_numpy(X_all).float()
    iszero = X_cpu == 0
    predict_dropout_mask = (best_PI_full > 0.5) & iszero
    X_imputed = torch.where(predict_dropout_mask, best_X_recon_full, X_cpu)

    return X_imputed.numpy()








